{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class PipelineRunner:\n",
    "    \"\"\"Boilerplate code for running a Dataflow pipelin.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_dataset, output_dir, project, pipeline_options=None):\n",
    "        self._tf_gcs_url = 'gs://cloud-datalab/deploy/tf/tensorflow-1.2.0-cp27-none-linux_x86_64.whl'\n",
    "        self._protobuf_gcs_url = 'gs://cloud-datalab/deploy/tf/protobuf-3.1.0-py2.py3-none-any.whl'\n",
    "        self._train_dataset = train_dataset\n",
    "        self._output_dir = output_dir\n",
    "        self._project = project\n",
    "        self._pipeline_options = pipeline_options\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Preprocess data in Cloud with DataFlow.\"\"\"\n",
    "        import mltoolbox.image.classification._util as util\n",
    "        from tensorflow.python.lib.io import file_io\n",
    "\n",
    "        tmpdir = tempfile.mkdtemp()\n",
    "        original_level = logging.getLogger().getEffectiveLevel()\n",
    "        logging.getLogger().setLevel(logging.ERROR)\n",
    "        try:\n",
    "            # Workaround for DataFlow 2.0, which doesn't work well with extra packages in GCS.\n",
    "            # Remove when the issue is fixed and new version of DataFlow is included in Datalab.\n",
    "            staging_package_url = util.repackage_to_staging(self._output_dir)\n",
    "            extra_packages = [staging_package_url, self._tf_gcs_url, self._protobuf_gcs_url]\n",
    "            local_packages = [os.path.join(tmpdir, os.path.basename(p)) for p in extra_packages]\n",
    "            for source, dest in zip(extra_packages, local_packages):\n",
    "                file_io.copy(source, dest, overwrite=True)\n",
    "            if self._pipeline_options is None:\n",
    "                additional_options = {}\n",
    "            else:\n",
    "                additional_options = dict(self._pipeline_options)\n",
    "            additional_options['extra_packages'] = local_packages\n",
    "\n",
    "            p = create_pipeline(self._train_dataset,\n",
    "                                self._output_dir,\n",
    "                                self._project,\n",
    "                                additional_options)\n",
    "            job_results = p.run()\n",
    "            dataflow_url = 'https://console.developers.google.com/dataflow?project=%s' % self._project\n",
    "            html = 'Job \"%s\" submitted.' % p.options.get_all_options()['job_name']\n",
    "            html += '<p>Click <a href=\"%s\" target=\"_blank\">here</a> to track preprocessing job. <br/>' % dataflow_url\n",
    "            IPython.display.display_html(html, raw=True)\n",
    "            job_results.wait_until_finish()\n",
    "        finally:\n",
    "            shutil.rmtree(tmpdir)\n",
    "            logging.getLogger().setLevel(original_level)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
