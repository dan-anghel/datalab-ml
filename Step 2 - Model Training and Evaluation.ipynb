{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Module training and evaluation\n",
    "\n",
    "### _TODO_\n",
    "This notebook continues the codifies the capabilities discussed in this [blog post](https://8081-dot-3124631-dot-devshell.appspot.com/). In a nutshell, it uses the pre-trained inception model as a starting point and then uses transfer learning to train it further on additional, customer-specific images. For explanation, simple flower images are used. Compared to training from scratch, the time and costs are drastically reduced.\n",
    "\n",
    "This notebook does preprocessing, training and prediction by calling CloudML API instead of running them in the Datalab container.  The purpose of local work is to do some initial prototyping and debugging on small scale data - often by taking a suitable (say 0.1 - 1%) sample of the full data. The same basic steps can then be repeated with much larger datasets in cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import variables\n",
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import signature_def_utils\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "from . import _inceptionlib\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "LOGITS_TENSOR_NAME = 'logits_tensor'\n",
    "IMAGE_URI_COLUMN = 'image_uri'\n",
    "LABEL_COLUMN = 'label'\n",
    "EMBEDDING_COLUMN = 'embedding'\n",
    "BOTTLENECK_TENSOR_SIZE = 2048\n",
    "\n",
    "\n",
    "class GraphMod(Enum):\n",
    "  TRAIN = 1\n",
    "  EVALUATE = 2\n",
    "  PREDICT = 3\n",
    "\n",
    "\n",
    "class GraphReferences(object):\n",
    "  \"\"\"Holder of base tensors used for training model using common task.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self.examples = None\n",
    "    self.train = None\n",
    "    self.global_step = None\n",
    "    self.metric_updates = []\n",
    "    self.metric_values = []\n",
    "    self.keys = None\n",
    "    self.predictions = []\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "  \"\"\"TensorFlow model for the flowers problem.\"\"\"\n",
    "\n",
    "  def __init__(self, labels, dropout, inception_checkpoint_file):\n",
    "    self.labels = labels\n",
    "    self.labels.sort()\n",
    "    self.dropout = dropout\n",
    "    self.inception_checkpoint_file = inception_checkpoint_file\n",
    "\n",
    "  def loss(loss_value):\n",
    "    \"\"\"Calculates aggregated mean loss.\"\"\"\n",
    "    total_loss = tf.Variable(0.0, False)\n",
    "    loss_count = tf.Variable(0, False)\n",
    "    total_loss_update = tf.assign_add(total_loss, loss_value)\n",
    "    loss_count_update = tf.assign_add(loss_count, 1)\n",
    "    loss_op = total_loss / tf.cast(loss_count, tf.float32)\n",
    "    return [total_loss_update, loss_count_update], loss_op\n",
    "\n",
    "  def accuracy(logits, labels):\n",
    "    \"\"\"Calculates aggregated accuracy.\"\"\"\n",
    "    is_correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    correct = tf.reduce_sum(tf.cast(is_correct, tf.int32))\n",
    "    incorrect = tf.reduce_sum(tf.cast(tf.logical_not(is_correct), tf.int32))\n",
    "    correct_count = tf.Variable(0, False)\n",
    "    incorrect_count = tf.Variable(0, False)\n",
    "    correct_count_update = tf.assign_add(correct_count, correct)\n",
    "    incorrect_count_update = tf.assign_add(incorrect_count, incorrect)\n",
    "    accuracy_op = tf.cast(correct_count, tf.float32) / tf.cast(\n",
    "        correct_count + incorrect_count, tf.float32)\n",
    "    return [correct_count_update, incorrect_count_update], accuracy_op\n",
    "\n",
    "  def decode_and_resize(image_str_tensor):\n",
    "  \"\"\"Decodes jpeg string, resizes it and returns a uint8 tensor.\"\"\"\n",
    "\n",
    "    # These constants are set by Inception v3's expectations.\n",
    "    height = 299\n",
    "    width = 299\n",
    "    channels = 3\n",
    "\n",
    "    image = tf.image.decode_jpeg(image_str_tensor, channels=channels)\n",
    "    # Note resize expects a batch_size, but tf_map supresses that index,\n",
    "    # thus we have to expand then squeeze.  Resize returns float32 in the\n",
    "    # range [0, uint8_max]\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    image = tf.image.resize_bilinear(image, [height, width], align_corners=False)\n",
    "    image = tf.squeeze(image, squeeze_dims=[0])\n",
    "    image = tf.cast(image, dtype=tf.uint8)\n",
    "    return image\n",
    "\n",
    "  def add_final_training_ops(self,\n",
    "                             embeddings,\n",
    "                             all_labels_count,\n",
    "                             bottleneck_tensor_size,\n",
    "                             hidden_layer_size=BOTTLENECK_TENSOR_SIZE / 4,\n",
    "                             dropout_keep_prob=None):\n",
    "    \"\"\"Adds a new softmax and fully-connected layer for training.\n",
    "\n",
    "     The set up for the softmax and fully-connected layers is based on:\n",
    "     https://tensorflow.org/versions/master/tutorials/mnist/beginners/index.html\n",
    "\n",
    "     This function can be customized to add arbitrary layers for\n",
    "     application-specific requirements.\n",
    "    Args:\n",
    "      embeddings: The embedding (bottleneck) tensor.\n",
    "      all_labels_count: The number of all labels including the default label.\n",
    "      bottleneck_tensor_size: The number of embeddings.\n",
    "      hidden_layer_size: The size of the hidden_layer. Roughtly, 1/4 of the\n",
    "                         bottleneck tensor size.\n",
    "      dropout_keep_prob: the percentage of activation values that are retained.\n",
    "    Returns:\n",
    "      softmax: The softmax or tensor. It stores the final scores.\n",
    "      logits: The logits tensor.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('input'):\n",
    "      bottleneck_input = tf.placeholder_with_default(\n",
    "          embeddings,\n",
    "          shape=[None, bottleneck_tensor_size],\n",
    "          name='ReshapeSqueezed')\n",
    "      bottleneck_with_no_gradient = tf.stop_gradient(bottleneck_input)\n",
    "\n",
    "      with tf.name_scope('Wx_plus_b'):\n",
    "        hidden = layers.fully_connected(bottleneck_with_no_gradient,\n",
    "                                        hidden_layer_size)\n",
    "        # We need a dropout when the size of the dataset is rather small.\n",
    "        if dropout_keep_prob:\n",
    "          hidden = tf.nn.dropout(hidden, dropout_keep_prob)\n",
    "        logits = layers.fully_connected(\n",
    "            hidden, all_labels_count, activation_fn=None)\n",
    "\n",
    "    softmax = tf.nn.softmax(logits, name='softmax')\n",
    "    return softmax, logits\n",
    "\n",
    "  def build_inception_graph(self):\n",
    "    \"\"\"Builds an inception graph and add the necessary input & output tensors.\n",
    "\n",
    "      To use other Inception models modify this file. Also preprocessing must be\n",
    "      modified accordingly.\n",
    "\n",
    "      See tensorflow/contrib/slim/python/slim/nets/inception_v3.py for\n",
    "      details about InceptionV3.\n",
    "\n",
    "    Returns:\n",
    "      input_jpeg: A placeholder for jpeg string batch that allows feeding the\n",
    "                  Inception layer with image bytes for prediction.\n",
    "      inception_embeddings: The embeddings tensor.\n",
    "    \"\"\"\n",
    "    image_str_tensor = tf.placeholder(tf.string, shape=[None])\n",
    "\n",
    "    # The CloudML Prediction API always \"feeds\" the Tensorflow graph with\n",
    "    # dynamic batch sizes e.g. (?,).  decode_jpeg only processes scalar\n",
    "    # strings because it cannot guarantee a batch of images would have\n",
    "    # the same output size.  We use tf.map_fn to give decode_jpeg a scalar\n",
    "    # string from dynamic batches.\n",
    "    image = tf.map_fn(decode_and_resize, image_str_tensor, back_prop=False, dtype=tf.uint8)\n",
    "    # convert_image_dtype, also scales [0, uint8_max] -> [0 ,1).\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "\n",
    "    # Then shift images to [-1, 1) for Inception.\n",
    "    image = tf.subtract(image, 0.5)\n",
    "    image = tf.multiply(image, 2.0)\n",
    "\n",
    "    # Build Inception layers, which expect A tensor of type float from [-1, 1)\n",
    "    # and shape [batch_size, height, width, channels].\n",
    "    with slim.arg_scope(_inceptionlib.inception_v3_arg_scope()):\n",
    "      _, end_points = _inceptionlib.inception_v3(image, is_training=False)\n",
    "\n",
    "    inception_embeddings = end_points['PreLogits']\n",
    "    inception_embeddings = tf.squeeze(\n",
    "        inception_embeddings, [1, 2], name='SpatialSqueeze')\n",
    "    return image_str_tensor, inception_embeddings\n",
    "\n",
    "  def build_graph(self, data_paths, batch_size, graph_mod):\n",
    "    \"\"\"Builds generic graph for training or eval.\"\"\"\n",
    "    tensors = GraphReferences()\n",
    "    is_training = graph_mod == GraphMod.TRAIN\n",
    "    if data_paths:\n",
    "      _, tensors.examples = _util.read_examples(\n",
    "          data_paths,\n",
    "          batch_size,\n",
    "          shuffle=is_training,\n",
    "          num_epochs=None if is_training else 2)\n",
    "    else:\n",
    "      tensors.examples = tf.placeholder(tf.string, name='input', shape=(None,))\n",
    "\n",
    "    if graph_mod == GraphMod.PREDICT:\n",
    "      inception_input, inception_embeddings = self.build_inception_graph()\n",
    "      # Build the Inception graph. We later add final training layers\n",
    "      # to this graph. This is currently used only for prediction.\n",
    "      # For training, we use pre-processed data, so it is not needed.\n",
    "      embeddings = inception_embeddings\n",
    "      tensors.input_jpeg = inception_input\n",
    "    else:\n",
    "      # For training and evaluation we assume data is preprocessed, so the\n",
    "      # inputs are tf-examples.\n",
    "      # Generate placeholders for examples.\n",
    "      with tf.name_scope('inputs'):\n",
    "        feature_map = {\n",
    "            'image_uri':\n",
    "                tf.FixedLenFeature(\n",
    "                    shape=[], dtype=tf.string, default_value=['']),\n",
    "            # Some images may have no labels. For those, we assume a default\n",
    "            # label. So the number of labels is label_count+1 for the default\n",
    "            # label.\n",
    "            'label':\n",
    "                tf.FixedLenFeature(\n",
    "                    shape=[1], dtype=tf.int64,\n",
    "                    default_value=[len(self.labels)]),\n",
    "            'embedding':\n",
    "                tf.FixedLenFeature(\n",
    "                    shape=[BOTTLENECK_TENSOR_SIZE], dtype=tf.float32)\n",
    "        }\n",
    "        parsed = tf.parse_example(tensors.examples, features=feature_map)\n",
    "        labels = tf.squeeze(parsed['label'])\n",
    "        uris = tf.squeeze(parsed['image_uri'])\n",
    "        embeddings = parsed['embedding']\n",
    "\n",
    "    # We assume a default label, so the total number of labels is equal to\n",
    "    # label_count+1.\n",
    "    all_labels_count = len(self.labels) + 1\n",
    "    with tf.name_scope('final_ops'):\n",
    "      softmax, logits = self.add_final_training_ops(\n",
    "          embeddings,\n",
    "          all_labels_count,\n",
    "          BOTTLENECK_TENSOR_SIZE,\n",
    "          dropout_keep_prob=self.dropout if is_training else None)\n",
    "\n",
    "    # Prediction is the index of the label with the highest score. We are\n",
    "    # interested only in the top score.\n",
    "    prediction = tf.argmax(softmax, 1)\n",
    "    tensors.predictions = [prediction, softmax, embeddings]\n",
    "\n",
    "    if graph_mod == GraphMod.PREDICT:\n",
    "      return tensors\n",
    "\n",
    "    with tf.name_scope('evaluate'):\n",
    "      loss_value = loss(logits, labels)\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    if is_training:\n",
    "      tensors.train, tensors.global_step = training(loss_value)\n",
    "    else:\n",
    "      tensors.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "      tensors.uris = uris\n",
    "\n",
    "    # Add means across all batches.\n",
    "    loss_updates, loss_op = _util.loss(loss_value)\n",
    "    accuracy_updates, accuracy_op = _util.accuracy(logits, labels)\n",
    "\n",
    "    if not is_training:\n",
    "      tf.summary.scalar('accuracy', accuracy_op)\n",
    "      tf.summary.scalar('loss', loss_op)\n",
    "\n",
    "    tensors.metric_updates = loss_updates + accuracy_updates\n",
    "    tensors.metric_values = [loss_op, accuracy_op]\n",
    "    return tensors\n",
    "\n",
    "  def build_train_graph(self, data_paths, batch_size):\n",
    "    return self.build_graph(data_paths, batch_size, GraphMod.TRAIN)\n",
    "\n",
    "  def build_eval_graph(self, data_paths, batch_size):\n",
    "    return self.build_graph(data_paths, batch_size, GraphMod.EVALUATE)\n",
    "\n",
    "  def restore_from_checkpoint(self, session, inception_checkpoint_file,\n",
    "                              trained_checkpoint_file):\n",
    "    \"\"\"To restore model variables from the checkpoint file.\n",
    "\n",
    "       The graph is assumed to consist of an inception model and other\n",
    "       layers including a softmax and a fully connected layer. The former is\n",
    "       pre-trained and the latter is trained using the pre-processed data. So\n",
    "       we restore this from two checkpoint files.\n",
    "    Args:\n",
    "      session: The session to be used for restoring from checkpoint.\n",
    "      inception_checkpoint_file: Path to the checkpoint file for the Inception\n",
    "                                 graph.\n",
    "      trained_checkpoint_file: path to the trained checkpoint for the other\n",
    "                               layers.\n",
    "    \"\"\"\n",
    "    inception_exclude_scopes = [\n",
    "        'InceptionV3/AuxLogits', 'InceptionV3/Logits', 'global_step',\n",
    "        'final_ops'\n",
    "    ]\n",
    "    reader = tf.train.NewCheckpointReader(inception_checkpoint_file)\n",
    "    var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "\n",
    "    # Get all variables to restore. Exclude Logits and AuxLogits because they\n",
    "    # depend on the input data and we do not need to intialize them.\n",
    "    all_vars = tf.contrib.slim.get_variables_to_restore(\n",
    "        exclude=inception_exclude_scopes)\n",
    "    # Remove variables that do not exist in the inception checkpoint (for\n",
    "    # example the final softmax and fully-connected layers).\n",
    "    inception_vars = {\n",
    "        var.op.name: var\n",
    "        for var in all_vars if var.op.name in var_to_shape_map\n",
    "    }\n",
    "    inception_saver = tf.train.Saver(inception_vars)\n",
    "    inception_saver.restore(session, inception_checkpoint_file)\n",
    "\n",
    "    # Restore the rest of the variables from the trained checkpoint.\n",
    "    trained_vars = tf.contrib.slim.get_variables_to_restore(\n",
    "        exclude=inception_exclude_scopes + inception_vars.keys())\n",
    "    trained_saver = tf.train.Saver(trained_vars)\n",
    "    trained_saver.restore(session, trained_checkpoint_file)\n",
    "\n",
    "  def build_prediction_graph(self):\n",
    "    \"\"\"Builds prediction graph and registers appropriate endpoints.\"\"\"\n",
    "\n",
    "    tensors = self.build_graph(None, 1, GraphMod.PREDICT)\n",
    "\n",
    "    keys_placeholder = tf.placeholder(tf.string, shape=[None])\n",
    "    inputs = {\n",
    "        'key': keys_placeholder,\n",
    "        'image_bytes': tensors.input_jpeg\n",
    "    }\n",
    "\n",
    "    # To extract the id, we need to add the identity function.\n",
    "    keys = tf.identity(keys_placeholder)\n",
    "    labels = self.labels + ['UNKNOWN']\n",
    "    labels_tensor = tf.constant(labels)\n",
    "    labels_table = tf.contrib.lookup.index_to_string_table_from_tensor(mapping=labels_tensor)\n",
    "    predicted_label = labels_table.lookup(tensors.predictions[0])\n",
    "\n",
    "    # Need to duplicate the labels by num_of_instances so the output is one batch\n",
    "    # (all output members share the same outer dimension).\n",
    "    # The labels are needed for client to match class scores list.\n",
    "    labels_tensor = tf.expand_dims(tf.constant(labels), 0)\n",
    "    num_instance = tf.shape(keys)\n",
    "    labels_tensors_n = tf.tile(labels_tensor, tf.concat(axis=0, values=[num_instance, [1]]))\n",
    "\n",
    "    outputs = {\n",
    "        'key': keys,\n",
    "        'prediction': predicted_label,\n",
    "        'labels': labels_tensors_n,\n",
    "        'scores': tensors.predictions[1],\n",
    "    }\n",
    "    return inputs, outputs\n",
    "\n",
    "  def export(self, last_checkpoint, output_dir):\n",
    "    \"\"\"Builds a prediction graph and xports the model.\n",
    "\n",
    "    Args:\n",
    "      last_checkpoint: Path to the latest checkpoint file from training.\n",
    "      output_dir: Path to the folder to be used to output the model.\n",
    "    \"\"\"\n",
    "    logging.info('Exporting prediction graph to %s', output_dir)\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "      # Build and save prediction meta graph and trained variable values.\n",
    "      inputs, outputs = self.build_prediction_graph()\n",
    "      signature_def_map = {\n",
    "        'serving_default': signature_def_utils.predict_signature_def(inputs, outputs)\n",
    "      }\n",
    "      init_op = tf.global_variables_initializer()\n",
    "      sess.run(init_op)\n",
    "      self.restore_from_checkpoint(sess, self.inception_checkpoint_file,\n",
    "                                   last_checkpoint)\n",
    "      init_op_serving = control_flow_ops.group(\n",
    "          variables.local_variables_initializer(),\n",
    "          tf.tables_initializer())\n",
    "\n",
    "      builder = saved_model_builder.SavedModelBuilder(output_dir)\n",
    "      builder.add_meta_graph_and_variables(\n",
    "          sess, [tag_constants.SERVING],\n",
    "          signature_def_map=signature_def_map,\n",
    "          legacy_init_op=init_op_serving)\n",
    "      builder.save(False)\n",
    "\n",
    "  def format_metric_values(self, metric_values):\n",
    "    \"\"\"Formats metric values - used for logging purpose.\"\"\"\n",
    "\n",
    "    # Early in training, metric_values may actually be None.\n",
    "    loss_str = 'N/A'\n",
    "    accuracy_str = 'N/A'\n",
    "    try:\n",
    "      loss_str = 'loss: %.3f' % metric_values[0]\n",
    "      accuracy_str = 'accuracy: %.3f' % metric_values[1]\n",
    "    except (TypeError, IndexError):\n",
    "      pass\n",
    "\n",
    "    return '%s, %s' % (loss_str, accuracy_str)\n",
    "\n",
    "  def format_prediction_values(self, prediction):\n",
    "    \"\"\"Formats prediction values - used for writing batch predictions as csv.\"\"\"\n",
    "    return '%.3f' % (prediction[0])\n",
    "\n",
    "\n",
    "def loss(logits, labels):\n",
    "  \"\"\"Calculates the loss from the logits and the labels.\n",
    "\n",
    "  Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size].\n",
    "  Returns:\n",
    "    loss: Loss tensor of type float.\n",
    "  \"\"\"\n",
    "  labels = tf.to_int64(labels)\n",
    "  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      logits=logits, labels=labels, name='xentropy')\n",
    "  return tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "\n",
    "def training(loss_op):\n",
    "  \"\"\"Calculates the loss from the logits and the labels.\n",
    "\n",
    "  Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size].\n",
    "  Returns:\n",
    "    loss: Loss tensor of type float.\n",
    "  \"\"\"\n",
    "  global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "  with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(epsilon=0.001)\n",
    "    train_op = optimizer.minimize(loss_op, global_step)\n",
    "    return train_op, global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "\n",
    "def get_train_eval_files(input_dir):\n",
    "  \"\"\"Get preprocessed training and eval files.\"\"\"\n",
    "  data_dir = _get_latest_data_dir(input_dir)\n",
    "  train_pattern = os.path.join(data_dir, 'train*.tfrecord.gz')\n",
    "  eval_pattern = os.path.join(data_dir, 'eval*.tfrecord.gz')\n",
    "  train_files = file_io.get_matching_files(train_pattern)\n",
    "  eval_files = file_io.get_matching_files(eval_pattern)\n",
    "  return train_files, eval_files\n",
    "\n",
    "\n",
    "def start_server(cluster, task):\n",
    "  if not task.type:\n",
    "    raise ValueError('--task_type must be specified.')\n",
    "  if task.index is None:\n",
    "    raise ValueError('--task_index must be specified.')\n",
    "\n",
    "  # Create and start a server.\n",
    "  return tf.train.Server(\n",
    "      tf.train.ClusterSpec(cluster),\n",
    "      protocol='grpc',\n",
    "      job_name=task.type,\n",
    "      task_index=task.index)\n",
    "\n",
    "class Evaluator(object):\n",
    "  \"\"\"Loads variables from latest checkpoint and performs model evaluation.\"\"\"\n",
    "\n",
    "  def __init__(self, model, data_paths, batch_size, output_path, dataset='eval'):\n",
    "    data_size = self._data_size(data_paths)\n",
    "    if data_size <= batch_size:\n",
    "      raise Exception('Data size is smaller than batch size.')\n",
    "    self.num_eval_batches = data_size // batch_size\n",
    "    self.batch_of_examples = []\n",
    "    self.checkpoint_path = os.path.join(output_path, 'train')\n",
    "    self.output_path = os.path.join(output_path, dataset)\n",
    "    self.eval_data_paths = data_paths\n",
    "    self.batch_size = batch_size\n",
    "    self.model = model\n",
    "\n",
    "  def _data_size(self, data_paths):\n",
    "    n = 0\n",
    "    options = tf.python_io.TFRecordOptions(\n",
    "        compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    for file in data_paths:\n",
    "      for line in tf.python_io.tf_record_iterator(file, options=options):\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "  def evaluate(self, num_eval_batches=None):\n",
    "    \"\"\"Run one round of evaluation, return loss and accuracy.\"\"\"\n",
    "\n",
    "    num_eval_batches = num_eval_batches or self.num_eval_batches\n",
    "    with tf.Graph().as_default() as graph:\n",
    "      self.tensors = self.model.build_eval_graph(self.eval_data_paths,\n",
    "                                                 self.batch_size)\n",
    "      self.summary = tf.summary.merge_all()\n",
    "      self.saver = tf.train.Saver()\n",
    "\n",
    "    self.summary_writer = tf.summary.FileWriter(self.output_path)\n",
    "    self.sv = tf.train.Supervisor(\n",
    "        graph=graph,\n",
    "        logdir=self.output_path,\n",
    "        summary_op=None,\n",
    "        global_step=None,\n",
    "        saver=self.saver)\n",
    "\n",
    "    last_checkpoint = tf.train.latest_checkpoint(self.checkpoint_path)\n",
    "    with self.sv.managed_session(master='', start_standard_services=False) as session:\n",
    "      self.sv.saver.restore(session, last_checkpoint)\n",
    "\n",
    "      if not self.batch_of_examples:\n",
    "        self.sv.start_queue_runners(session)\n",
    "        for i in range(num_eval_batches):\n",
    "          self.batch_of_examples.append(session.run(self.tensors.examples))\n",
    "\n",
    "      for i in range(num_eval_batches):\n",
    "        session.run(self.tensors.metric_updates,\n",
    "                    {self.tensors.examples: self.batch_of_examples[i]})\n",
    "\n",
    "      metric_values = session.run(self.tensors.metric_values)\n",
    "      global_step = tf.train.global_step(session, self.tensors.global_step)\n",
    "      summary = session.run(self.summary)\n",
    "      self.summary_writer.add_summary(summary, global_step)\n",
    "      self.summary_writer.flush()\n",
    "      return metric_values\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "  \"\"\"Performs model training and optionally evaluation.\"\"\"\n",
    "\n",
    "  def __init__(self, input_dir, batch_size, max_steps, output_path, model, cluster, task):\n",
    "    train_files, eval_files = get_train_eval_files(input_dir)\n",
    "    self.train_data_paths = train_files\n",
    "    self.output_path = output_path\n",
    "    self.batch_size = batch_size\n",
    "    self.model = model\n",
    "    self.max_steps = max_steps\n",
    "    self.cluster = cluster\n",
    "    self.task = task\n",
    "    self.evaluator = Evaluator(self.model, eval_files, batch_size, output_path, 'eval_set')\n",
    "    self.train_evaluator = Evaluator(self.model, train_files, batch_size, output_path, 'train_set')\n",
    "    self.min_train_eval_rate = 8\n",
    "\n",
    "  def run_training(self):\n",
    "    \"\"\"Runs a Master.\"\"\"\n",
    "    self.train_path = os.path.join(self.output_path, 'train')\n",
    "    self.model_path = os.path.join(self.output_path, 'model')\n",
    "    self.is_master = self.task.type != 'worker'\n",
    "    log_interval = 15\n",
    "    self.eval_interval = 30\n",
    "    if self.is_master and self.task.index > 0:\n",
    "      raise Exception('Only one replica of master expected')\n",
    "\n",
    "    if self.cluster:\n",
    "      logging.info('Starting %s/%d', self.task.type, self.task.index)\n",
    "      server = start_server(self.cluster, self.task)\n",
    "      target = server.target\n",
    "      device_fn = tf.train.replica_device_setter(\n",
    "          ps_device='/job:ps',\n",
    "          worker_device='/job:%s/task:%d' % (self.task.type, self.task.index),\n",
    "          cluster=self.cluster)\n",
    "      # We use a device_filter to limit the communication between this job\n",
    "      # and the parameter servers, i.e., there is no need to directly\n",
    "      # communicate with the other workers; attempting to do so can result\n",
    "      # in reliability problems.\n",
    "      device_filters = [\n",
    "          '/job:ps', '/job:%s/task:%d' % (self.task.type, self.task.index)\n",
    "      ]\n",
    "      config = tf.ConfigProto(device_filters=device_filters)\n",
    "    else:\n",
    "      target = ''\n",
    "      device_fn = ''\n",
    "      config = None\n",
    "\n",
    "    with tf.Graph().as_default() as graph:\n",
    "      with tf.device(device_fn):\n",
    "        # Build the training graph.\n",
    "        self.tensors = self.model.build_train_graph(self.train_data_paths,\n",
    "                                                    self.batch_size)\n",
    "\n",
    "        # Add the variable initializer Op.\n",
    "        init_op = tf.global_variables_initializer()\n",
    "\n",
    "        # Create a saver for writing training checkpoints.\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # Build the summary operation based on the TF collection of Summaries.\n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    # Create a \"supervisor\", which oversees the training process.\n",
    "    self.sv = tf.train.Supervisor(\n",
    "        graph,\n",
    "        is_chief=self.is_master,\n",
    "        logdir=self.train_path,\n",
    "        init_op=init_op,\n",
    "        saver=self.saver,\n",
    "        # Write summary_ops by hand.\n",
    "        summary_op=None,\n",
    "        global_step=self.tensors.global_step,\n",
    "        # No saving; we do it manually in order to easily evaluate immediately\n",
    "        # afterwards.\n",
    "        save_model_secs=0)\n",
    "\n",
    "    should_retry = True\n",
    "    to_run = [self.tensors.global_step, self.tensors.train]\n",
    "\n",
    "    while should_retry:\n",
    "      try:\n",
    "        should_retry = False\n",
    "        with self.sv.managed_session(target, config=config) as session:\n",
    "          self.start_time = start_time = time.time()\n",
    "          self.last_save = self.last_log = 0\n",
    "          self.global_step = self.last_global_step = 0\n",
    "          self.local_step = self.last_local_step = 0\n",
    "          self.last_global_time = self.last_local_time = start_time\n",
    "\n",
    "          # Loop until the supervisor shuts down or max_steps have\n",
    "          # completed.\n",
    "          max_steps = self.max_steps\n",
    "          while not self.sv.should_stop() and self.global_step < max_steps:\n",
    "            try:\n",
    "              # Run one step of the model.\n",
    "              self.global_step = session.run(to_run)[0]\n",
    "              self.local_step += 1\n",
    "\n",
    "              self.now = time.time()\n",
    "              is_time_to_eval = (self.now - self.last_save) > self.eval_interval\n",
    "              is_time_to_log = (self.now - self.last_log) > log_interval\n",
    "              should_eval = self.is_master and is_time_to_eval\n",
    "              should_log = is_time_to_log or should_eval\n",
    "\n",
    "              if should_log:\n",
    "                self.log(session)\n",
    "\n",
    "              if should_eval:\n",
    "                self.eval(session)\n",
    "            except tf.errors.AbortedError:\n",
    "              should_retry = True\n",
    "\n",
    "          if self.is_master:\n",
    "            # Take the final checkpoint and compute the final accuracy.\n",
    "            # self.saver.save(session, self.sv.save_path, self.tensors.global_step)\n",
    "            self.eval(session)\n",
    "\n",
    "      except tf.errors.AbortedError:\n",
    "        print('Hitting an AbortedError. Trying it again.')\n",
    "        should_retry = True\n",
    "\n",
    "    # Export the model for inference.\n",
    "    if self.is_master:\n",
    "      self.model.export(tf.train.latest_checkpoint(self.train_path), self.model_path)\n",
    "\n",
    "    # Ask for all the services to stop.\n",
    "    self.sv.stop()\n",
    "\n",
    "  def log(self, session):\n",
    "    \"\"\"Logs training progress.\"\"\"\n",
    "    logging.info('Train [%s/%d], step %d (%.3f sec) %.1f '\n",
    "                 'global steps/s, %.1f local steps/s', self.task.type,\n",
    "                 self.task.index, self.global_step,\n",
    "                 (self.now - self.start_time),\n",
    "                 (self.global_step - self.last_global_step) /\n",
    "                 (self.now - self.last_global_time),\n",
    "                 (self.local_step - self.last_local_step) /\n",
    "                 (self.now - self.last_local_time))\n",
    "    self.last_log = self.now\n",
    "    self.last_global_step, self.last_global_time = self.global_step, self.now\n",
    "    self.last_local_step, self.last_local_time = self.local_step, self.now\n",
    "\n",
    "  def eval(self, session):\n",
    "    \"\"\"Runs evaluation loop.\"\"\"\n",
    "    eval_start = time.time()\n",
    "    self.saver.save(session, self.sv.save_path, self.tensors.global_step)\n",
    "    logging.info(\n",
    "        'Eval, step %d:\\n- on train set %s\\n-- on eval set %s',\n",
    "        self.global_step,\n",
    "        self.model.format_metric_values(self.train_evaluator.evaluate()),\n",
    "        self.model.format_metric_values(self.evaluator.evaluate()))\n",
    "    now = time.time()\n",
    "\n",
    "    # Make sure eval doesn't consume too much of total time.\n",
    "    eval_time = now - eval_start\n",
    "    train_eval_rate = self.eval_interval / eval_time\n",
    "    if train_eval_rate < self.min_train_eval_rate and self.last_save > 0:\n",
    "      logging.info('Adjusting eval interval from %.2fs to %.2fs',\n",
    "                   self.eval_interval, self.min_train_eval_rate * eval_time)\n",
    "      self.eval_interval = self.min_train_eval_rate * eval_time\n",
    "\n",
    "    self.last_save = now\n",
    "    self.last_log = now\n",
    "\n",
    "  def save_summaries(self, session):\n",
    "    self.sv.summary_computed(session, session.run(self.summary_op), self.global_step)\n",
    "    self.sv.summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from google.datalab.utils import LambdaJob\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "# Configuration\n",
    "project_id = datalab_project_id()\n",
    "bucket = 'gs://candies-{}-dantest'.format(project_id)\n",
    "preprocess_dir = '{}/candies_preprocessed_cloud'.format(bucket)\n",
    "model_dir = '{}/candies_model_cloud'.format(bucket)\n",
    "staging_dir = '{}/staging'.format(bucket)\n",
    "\n",
    "# Load labels\n",
    "latest_file = os.path.join(preprocess_dir, 'latest')\n",
    "with file_io.FileIO(latest_file, 'r') as f:\n",
    "  dir_name = f.read().rstrip()\n",
    "data_dir = os.path.join(input_dir, dir_name)\n",
    "labels_file = os.path.join(data_dir, 'labels')\n",
    "with file_io.FileIO(labels_file, 'r') as f:\n",
    "  labels = f.read().rstrip().split('\\n')\n",
    "\n",
    "# Create the model and run the training\n",
    "checkpoint = 'gs://cloud-ml-data/img/flower_photos/inception_v3_2016_08_28.ckpt'\n",
    "model = Model(labels, 0.5, checkpoint)\n",
    "task_data = {'type': 'master', 'index': 0}\n",
    "task = type('TaskSpec', (object,), task_data)\n",
    "trainer = Trainer(preprocess_dir, 30, 800, model_dir, model, None, task)\n",
    "training_job = LambdaJob(lambda: trainer.run_training(), 'training')\n",
    "training_job.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
